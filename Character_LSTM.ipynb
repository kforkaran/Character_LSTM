{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int2char maps integers to characters\n",
    "# char2int maps characters to unique integers\n",
    "# chars contain all unique character present in text\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 52, 66, 51, 43, 46, 69, 57, 60, 17, 17, 17, 41, 66, 51, 51, 39,\n",
       "       57, 28, 66, 19, 48, 12, 48, 46,  1, 57, 66, 69, 46, 57, 66, 12, 12,\n",
       "       57, 66, 12, 48, 34, 46, 20, 57, 46, 11, 46, 69, 39, 57,  2, 13, 52,\n",
       "       66, 51, 51, 39, 57, 28, 66, 19, 48, 12, 39, 57, 48,  1, 57,  2, 13,\n",
       "       52, 66, 51, 51, 39, 57, 48, 13, 57, 48, 43,  1, 57, 26, 27, 13, 17,\n",
       "       27, 66, 39, 59, 17, 17,  3, 11, 46, 69, 39, 43, 52, 48, 13])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Filling the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Reshaping it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    n_batches = len(arr)//(batch_size * seq_length)\n",
    "    \n",
    "    ## Keeping only enough characters to make full batches\n",
    "    arr = arr[:n_batches * (batch_size * seq_length)]\n",
    "    \n",
    "    ## Reshaping into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    ## Iterating over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# checking if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "        '''\n",
    "                \n",
    "        ## outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "\n",
    "        # Stacking up LSTM outputs using view\n",
    "        # contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encoding the data and making them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # loss and backpropagation\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # clip_grad_norm prevent the exploding gradient problem\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 50... Loss: 3.1440... Val Loss: 3.1161\n",
      "Epoch: 1/20... Step: 100... Loss: 3.0646... Val Loss: 3.0531\n",
      "Epoch: 2/20... Step: 150... Loss: 2.5912... Val Loss: 2.5431\n",
      "Epoch: 2/20... Step: 200... Loss: 2.3842... Val Loss: 2.3571\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2297... Val Loss: 2.2308\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1484... Val Loss: 2.1140\n",
      "Epoch: 3/20... Step: 350... Loss: 2.0716... Val Loss: 2.0269\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9488... Val Loss: 1.9462\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8644... Val Loss: 1.8782\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8640... Val Loss: 1.8245\n",
      "Epoch: 4/20... Step: 550... Loss: 1.8250... Val Loss: 1.7696\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7367... Val Loss: 1.7322\n",
      "Epoch: 5/20... Step: 650... Loss: 1.7080... Val Loss: 1.6892\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6733... Val Loss: 1.6560\n",
      "Epoch: 6/20... Step: 750... Loss: 1.6129... Val Loss: 1.6278\n",
      "Epoch: 6/20... Step: 800... Loss: 1.6254... Val Loss: 1.6051\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5832... Val Loss: 1.5798\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5618... Val Loss: 1.5560\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5482... Val Loss: 1.5385\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5298... Val Loss: 1.5168\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.5014... Val Loss: 1.5051\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4733... Val Loss: 1.4842\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.5077... Val Loss: 1.4767\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4423... Val Loss: 1.4635\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4435... Val Loss: 1.4479\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4344... Val Loss: 1.4395\n",
      "Epoch: 10/20... Step: 1350... Loss: 1.3989... Val Loss: 1.4308\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4459... Val Loss: 1.4178\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3639... Val Loss: 1.4113\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3759... Val Loss: 1.4024\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.4092... Val Loss: 1.3904\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3685... Val Loss: 1.3857\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3356... Val Loss: 1.3829\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3489... Val Loss: 1.3695\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3023... Val Loss: 1.3715\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3469... Val Loss: 1.3595\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2802... Val Loss: 1.3529\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3263... Val Loss: 1.3521\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3172... Val Loss: 1.3469\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2821... Val Loss: 1.3419\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.2953... Val Loss: 1.3344\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2878... Val Loss: 1.3282\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.3130... Val Loss: 1.3213\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2708... Val Loss: 1.3211\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2605... Val Loss: 1.3170\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2373... Val Loss: 1.3100\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2753... Val Loss: 1.3027\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2774... Val Loss: 1.3023\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2321... Val Loss: 1.2949\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2346... Val Loss: 1.2932\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2222... Val Loss: 1.2894\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2239... Val Loss: 1.2904\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2417... Val Loss: 1.2801\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2331... Val Loss: 1.2785\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2131... Val Loss: 1.2776\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs =  20\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priming and generating text \n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna shidter\n",
      "in a crowd and the sort of happiless, and started at the merching the\n",
      "servorm.\n",
      "\n",
      "The memories of the sacrace of the meeting had seen some moments, sens and\n",
      "to be sure that she should go to him to him. The carriage had no means to\n",
      "him. The more had been delighted to him, so to say.\n",
      "\n",
      "At the recovinions of his brother was so many. And she went to a\n",
      "smile, but the same time their wealth and the possible carriage as though\n",
      "something was standing, when the same ashement of his sort, with his friend,\n",
      "soonef, and said that he had succeeded him to their sense of socance of\n",
      "the sort.\n",
      "\n",
      "\"It's true whom I wanted to be in the frait of his brother--and his wife\n",
      "was so lest found. How are you are free only one of them the carriages and\n",
      "some ow that you, there'r noting to step the carryange.\"\n",
      "\n",
      "\"Well it was to bring it?\"\n",
      "\n",
      "\"Yes, but that's in your suncertions.\"\n",
      "\n",
      "She was not tines of that with their closs, who would have a man\n",
      "and what they saw that she did not see the proplexed of a long wais, but\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said\n",
      "at the presens of with a little carriage; \"but will be, and all the\n",
      "same sort of sinceress is no old standing of it. Why should I do be\n",
      "nothing on to be districtly one of her, but the linent moment, and\n",
      "see you are so stalling for you, I shall be nothing?\"\n",
      "\n",
      "She was going about the colonel at the most farsh of a\n",
      "soft of touch of the same to the same on the part, but at tame in\n",
      "the more itself telling his sense of without together were such all\n",
      "though seemed so much there, when he could not see them, and they heard\n",
      "a married men. Soness turned off to the sound, the conversation took the\n",
      "chollence of home, but his wife wished a single softer,\n",
      "he so as he was not talking of herself, and his brother was strange\n",
      "feeling.\n",
      "\n",
      "She had at a smile smile.\n",
      "\n",
      "\"The peasants. The country ond host was a strangel of myself immense that he's so\n",
      "district, thinks of the position.\"\n",
      "\n",
      "\"You want to say.\"\n",
      "\n",
      "\"I don't know, I'll go to her in her, and the carriage of insteast in\n",
      "those teres. There is the past in which his side was the man, and\n",
      "the sort of surposition of myself in a soul of home, and\n",
      "the charactery is the familiar figure of my present patting in the count him of my\n",
      "position was in the partices and time in the souper.\n",
      "\n",
      "\"You can't be attempted in the same to me for a money for the part of his\n",
      "such man who had saying your charm,\" she said.\n",
      "\n",
      "\"Why's a soliticular of this death of them all to say to the\n",
      "sacrament.\"\n",
      "\n",
      "\"I am very glad, I've been triumphing for the position to me. I could not\n",
      "believe that it was so deples, and then all the tones of her mere whom\n",
      "here, what ament and a sort in the second business, and they are\n",
      "to take him in official strest. And at it, it's a sort of sill friend of\n",
      "things,\" said Vronsky's sister-in-low as he had broken the box and\n",
      "talking that her let on his brother's hands, and turning his father. He\n",
      "said a loud lips, who had satisfied the place. She was the charm,\n",
      "and asking her to tear, had as she were struck a meaning of a step, and\n",
      "was simply fashen, he w\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
